{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma fine-tuner app\n",
        "\n",
        "This is a gradio app that one can use to fine tune any Gemma 3 model with their own dataset.\n",
        "\n",
        "## How to use\n",
        "\n",
        "1. Change the runtime type to \"T4 GPU\"\n",
        "2. Run all the cells till the last one\n",
        "3. The output of the last cell will contain a link, just click on it to visit the Gradio app."
      ],
      "metadata": {
        "id": "CejSgq4I4ZF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/inclinedadarsh/gemma-finetune-ui/blob/main/notebooks/app/gemma-finetune-gradio.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "weoMSyaS5GOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jII7vIOD3KG_"
      },
      "outputs": [],
      "source": [
        "%pip install -U transformers datasets peft trl bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "ASFLArc93T7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OPTIONS = [\n",
        "    \"google/gemma-3-1b-it\", \"google/gemma-3-1b-pt\",\n",
        "    \"google/gemma-3-4b-it\", \"google/gemma-3-4b-pt\",\n",
        "    \"google/gemma-3-12b-it\", \"google/gemma-3-12b-pt\",\n",
        "    \"google/gemma-3-27b-it\", \"google/gemma-3-27b-pt\"\n",
        "]"
      ],
      "metadata": {
        "id": "HgNtNdZL34eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_fn(model_name, load_4bit, hf_token):\n",
        "    logs = []\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    if not hf_token.strip():\n",
        "        logs.append(\"Error: Please enter Hugging Face token.\")\n",
        "        return \"\\n\".join(logs), None, None\n",
        "\n",
        "    logs.append(f\"Starting to load model: {model_name}...\")\n",
        "    time.sleep(0.5)\n",
        "    try:\n",
        "        if load_4bit:\n",
        "            logs.append(\"4-bit option selected. Setting up BitsAndBytes config...\")\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type='nf4',\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_quant_storage=torch.float16\n",
        "            )\n",
        "            logs.append(\"Loading model with 4-bit quantization. This may take a while...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                attn_implementation='eager',\n",
        "                quantization_config=bnb_config,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map='auto',\n",
        "                token=hf_token\n",
        "            )\n",
        "        else:\n",
        "            logs.append(\"Loading model without 4-bit quantization...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                attn_implementation='eager',\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map='auto',\n",
        "                token=hf_token\n",
        "            )\n",
        "\n",
        "        for percent in range(10, 101, 30):\n",
        "            time.sleep(0.5)\n",
        "            logs.append(f\"Model loading progress: {percent}%...\")\n",
        "\n",
        "        logs.append(\"Model loaded successfully.\")\n",
        "        logs.append(\"Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "        logs.append(\"Tokenizer loaded successfully.\")\n",
        "        logs.append(\"Gemma model and tokenizer are ready for use.\")\n",
        "        return \"\\n\".join(logs), model, tokenizer\n",
        "    except Exception as e:\n",
        "        logs.append(f\"Error loading model: {str(e)}\")\n",
        "        return \"\\n\".join(logs), None, None"
      ],
      "metadata": {
        "id": "G2FYP8AT4AZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_dataset(file):\n",
        "    if file is None:\n",
        "        return None\n",
        "    try:\n",
        "        # Handle file object or path string\n",
        "        file_path = file[\"name\"] if isinstance(file, dict) and \"name\" in file else file\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'user' not in df.columns or 'assistant' not in df.columns:\n",
        "            return pd.DataFrame({\"Error\": [\"CSV must have 'user' and 'assistant' columns.\"]})\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading CSV: {str(e)}\"]})"
      ],
      "metadata": {
        "id": "ZJc4ulOmDhWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(file, system_message):\n",
        "    if file is None:\n",
        "        return {\"error\": \"No file uploaded.\"}, None\n",
        "    try:\n",
        "        file_path = file[\"name\"] if isinstance(file, dict) and \"name\" in file else file\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'user' not in df.columns or 'assistant' not in df.columns:\n",
        "            return {\"error\": \"CSV must have 'user' and 'assistant' columns.\"}, None\n",
        "\n",
        "        ds = Dataset.from_pandas(df)\n",
        "\n",
        "        def format_example(example):\n",
        "            return {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": example['user']},\n",
        "                    {\"role\": \"assistant\", \"content\": example['assistant']}\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        ds = ds.map(format_example, remove_columns=ds.column_names, batched=False)\n",
        "        sample = ds[0]\n",
        "        return sample, ds\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error processing dataset: {str(e)}\"}, None"
      ],
      "metadata": {
        "id": "08fpECgRDqIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_fn(model, tokenizer, dataset, use_lora, rank, alpha, dropout, epochs, learning_rate, max_seq_length, optim, push_to_hub, repo_id, hf_token):\n",
        "    logs = []\n",
        "    if model is None:\n",
        "        return \"Error: Model not loaded.\", None\n",
        "    if tokenizer is None:\n",
        "        return \"Error: Tokenizer not loaded.\", None\n",
        "    if dataset is None:\n",
        "        return \"Error: Processed dataset not loaded.\", None\n",
        "\n",
        "    logs.append(\"Starting fine tuning process...\")\n",
        "    if use_lora:\n",
        "        from peft import LoraConfig\n",
        "        peft_config = LoraConfig(\n",
        "            lora_alpha=alpha,\n",
        "            lora_dropout=dropout,\n",
        "            r=rank,\n",
        "            bias=\"none\",\n",
        "            target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        logs.append(\"LoRA configuration set.\")\n",
        "    else:\n",
        "        peft_config = None\n",
        "        logs.append(\"LoRA not used. Proceeding without PEFT configuration.\")\n",
        "\n",
        "    # Build training configuration (with fixed parameters for this prototype)\n",
        "    from trl import SFTConfig\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=\"./gemma-finetune\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        packing=True,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=optim,\n",
        "        logging_steps=2,\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=learning_rate,\n",
        "        fp16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type='constant',\n",
        "        report_to='none',\n",
        "        dataset_kwargs={\n",
        "            \"add_special_tokens\": False,\n",
        "            \"append_concat_token\": True\n",
        "        },\n",
        "        push_to_hub=push_to_hub,\n",
        "        hub_model_id=repo_id,\n",
        "        hub_token=hf_token\n",
        "    )\n",
        "    logs.append(\"Training configuration set. Starting trainer...\")\n",
        "\n",
        "    from trl import SFTTrainer\n",
        "    try:\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            peft_config=peft_config,\n",
        "            processing_class=tokenizer,\n",
        "            train_dataset=dataset\n",
        "        )\n",
        "        logs.append(\"Training started...\")\n",
        "        trainer.train()\n",
        "        logs.append(\"Training complete.\")\n",
        "        return \"\\n\".join(logs), trainer\n",
        "    except Exception as e:\n",
        "        return f\"Error during training: {str(e)}\", None\n"
      ],
      "metadata": {
        "id": "jRfHAJbqSw1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_hub_fn(trainer, repo_id):\n",
        "    if trainer is None:\n",
        "        return \"Error: Model not trained yet.\"\n",
        "    if not trainer.push_to_hub:\n",
        "        return \"Error: Push to Hub wasn't enabled!\"\n",
        "    try:\n",
        "        trainer.push_to_hub()\n",
        "        return f\"Model successfully pushed to the Hugging Face Hub at {repo_id}!\"\n",
        "    except Exception as e:\n",
        "        return f\"Error pushing model to hub: {str(e)}\""
      ],
      "metadata": {
        "id": "tF22v64ejoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Gemma Model Loader, Dataset Processor, Fine Tuner & Hub Pusher\")\n",
        "    gr.Markdown(\"Use the sidebar for model configuration and system message. Then upload and process a CSV dataset, fine tune the model, and finally push it to the Hugging Face Hub.\")\n",
        "\n",
        "    model_state = gr.State()\n",
        "    tokenizer_state = gr.State()\n",
        "    dataset_state = gr.State()\n",
        "    trained_model_state = gr.State()\n",
        "\n",
        "    with gr.Sidebar():\n",
        "        gr.Markdown(\"## Model Configuration\")\n",
        "        model_dropdown = gr.Dropdown(choices=MODEL_OPTIONS, label=\"Select Model\", value=MODEL_OPTIONS[0])\n",
        "        load_4bit_checkbox = gr.Checkbox(label=\"Load model in 4 bit? (Saves ton of memory)\", value=False)\n",
        "        hf_token_input = gr.Textbox(label=\"Hugging Face Token\", placeholder=\"Enter your Hugging Face token\", type=\"password\")\n",
        "        gr.Markdown(\"## System Message\")\n",
        "        system_message_input = gr.Textbox(label=\"System Message\", placeholder=\"Enter system message here\")\n",
        "        load_model_button = gr.Button(\"Load Model\")\n",
        "\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"### Model Loading Log\")\n",
        "        model_log = gr.Textbox(label=\"Loading Log\", lines=15)\n",
        "        load_model_button.click(load_model_fn,\n",
        "                                inputs=[model_dropdown, load_4bit_checkbox, hf_token_input],\n",
        "                                outputs=[model_log, model_state, tokenizer_state])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"## Dataset Loader\")\n",
        "        csv_file = gr.File(label=\"Upload CSV\", file_types=['.csv'])\n",
        "        dataset_preview = gr.Dataframe(label=\"Dataset Preview\")\n",
        "        process_button = gr.Button(\"Process Dataset\")\n",
        "        processed_output = gr.JSON(label=\"Sample Processed Output\")\n",
        "\n",
        "        csv_file.change(fn=display_dataset, inputs=csv_file, outputs=dataset_preview)\n",
        "\n",
        "        process_button.click(process_dataset,\n",
        "                             inputs=[csv_file, system_message_input],\n",
        "                             outputs=[processed_output, dataset_state])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"## Fine Tuning Configuration\")\n",
        "        with gr.Row():\n",
        "            use_lora_checkbox = gr.Checkbox(label=\"Use LoRA\", value=False)\n",
        "            rank_input = gr.Number(label=\"LoRA Rank (r)\", value=8)\n",
        "            alpha_input = gr.Number(label=\"LoRA Alpha\", value=8)\n",
        "            dropout_input = gr.Number(label=\"LoRA Dropout\", value=0.05)\n",
        "        with gr.Row():\n",
        "            epochs_input = gr.Number(label=\"Epochs\", value=3)\n",
        "            lr_input = gr.Number(label=\"Learning Rate\", value=2e-4)\n",
        "        with gr.Row():\n",
        "            max_seq_length_input = gr.Number(label=\"Max Seq Length\", value=512)\n",
        "            optim_dropdown = gr.Dropdown(label=\"Optimizer\", choices=[\"adamw_torch_fused\", \"adamw\"], value=\"adamw_torch_fused\")\n",
        "        with gr.Row():\n",
        "            push_to_hub_checkbox = gr.Checkbox(label=\"Push to Hub\", value=False)\n",
        "            repo_name_input = gr.Textbox(label=\"Repository Name\", placeholder=\"Enter repository name where the model should be pushed\")\n",
        "\n",
        "        training_log = gr.Textbox(label=\"Training Log\", lines=15)\n",
        "        start_training_button = gr.Button(\"Start Training\")\n",
        "\n",
        "        start_training_button.click(fine_tune_fn,\n",
        "                                    inputs=[model_state, tokenizer_state, dataset_state,\n",
        "                                            use_lora_checkbox, rank_input, alpha_input, dropout_input,\n",
        "                                            epochs_input, lr_input, max_seq_length_input, optim_dropdown, push_to_hub_checkbox, repo_name_input, hf_token_input],\n",
        "                                    outputs=[training_log, trained_model_state])\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"## Push Fine-Tuned Model to Hugging Face Hub\")\n",
        "        push_to_hub_button = gr.Button(\"Push to Hub\")\n",
        "        push_log = gr.Textbox(label=\"Push Log\", lines=8)\n",
        "        push_to_hub_button.click(push_to_hub_fn,\n",
        "                                 inputs=[trained_model_state, repo_name_input],\n",
        "                                 outputs=push_log)"
      ],
      "metadata": {
        "id": "lrEtdBgoD12F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "OBgmt2YS4jX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4EtjuKkxal2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}