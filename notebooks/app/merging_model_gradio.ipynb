{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio app for merging and converting HF model to GGUF model\n",
        "\n",
        "This app runs a gradio notebook to load model, merge it if needed and then convert it into GGUF format that can be downloaded.\n",
        "\n",
        "This was first implemented in a notebook, you can check it out here: https://github.com/inclinedadarsh/gemma-finetune-ui/blob/main/notebooks/merging_model.ipynb\n",
        "\n",
        "To run this app, open it in colab, and run all the cells till the last one, and open the link given in the output.\n",
        "\n",
        "> Make sure to change the runtime type to **T4 GPU**"
      ],
      "metadata": {
        "id": "el_s-2PgcFLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/inclinedadarsh/gemma-finetune-ui/blob/main/notebooks/merging_model_gradio.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "vC0RjPQ0ga9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-tYgSaqQPsrE"
      },
      "outputs": [],
      "source": [
        "%pip install -U gradio transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "fH4Ihm9VSIE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "44j3483zS4Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "8EYSLrH7PybD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_model(model_id, model_is_merged):\n",
        "    \"\"\"\n",
        "    If the model is not merged, this function loads the PEFT model,\n",
        "    merges it with its base model, and saves the merged model to './merged_model'.\n",
        "    If the model is already merged, it skips merging.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if model_is_merged:\n",
        "            return \"Merge skipped. Using existing merged model folder: './merged_model'\"\n",
        "        else:\n",
        "            peft_config = PeftConfig.from_pretrained(model_id)\n",
        "            base_model_id = peft_config.base_model_name_or_path\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model_id,\n",
        "                device_map='auto',\n",
        "                torch_dtype=torch.float16,\n",
        "                attn_implementation='eager'\n",
        "            )\n",
        "\n",
        "            model = PeftModel.from_pretrained(base_model, model_id)\n",
        "\n",
        "            merged_model = model.merge_and_unload()\n",
        "\n",
        "            os.makedirs(\"./merged_model\", exist_ok=True)\n",
        "            merged_model.save_pretrained('./merged_model')\n",
        "            tokenizer.save_pretrained(\"./merged_model\")\n",
        "            return \"Merging completed and saved to './merged_model'\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during merging: {e}\""
      ],
      "metadata": {
        "id": "Z1yhZeAQQRV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_gguf():\n",
        "    \"\"\"\n",
        "    This function clones the llama.cpp repository (if needed) and runs the conversion\n",
        "    from the merged Hugging Face model (in './merged_model') to GGUF format.\n",
        "    The output GGUF file is saved in './merged_model/merged_model.gguf' and its path is returned.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(\"llama.cpp\"):\n",
        "            os.system(\"git clone https://github.com/ggerganov/llama.cpp.git\")\n",
        "\n",
        "        orig_dir = os.getcwd()\n",
        "        os.chdir(\"llama.cpp\")\n",
        "\n",
        "        merged_model_path = os.path.join(\"..\", \"merged_model\")\n",
        "        outfile = os.path.join(\"..\", \"merged_model\", \"merged_model.gguf\")\n",
        "\n",
        "        conversion_command = f\"python convert_hf_to_gguf.py {merged_model_path} --outfile {outfile}\"\n",
        "        os.system(conversion_command)\n",
        "\n",
        "        os.chdir(orig_dir)\n",
        "\n",
        "        if os.path.exists('./merged_model/merged_model.gguf'):\n",
        "            return './merged_model/merged_model.gguf'\n",
        "        else:\n",
        "            return \"Conversion finished but the output file was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during conversion: {e}\""
      ],
      "metadata": {
        "id": "oM9p4BFMQn-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Hugging Face Model Merger and GGUF Converter\")\n",
        "\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"## Step 1: Merge Model\")\n",
        "        model_id = gr.Textbox(label=\"Hugging Face Model ID\",\n",
        "                              placeholder=\"e.g., inclinedadarsh/gemma-3-1b-nl-to-regex\")\n",
        "        model_is_merged = gr.Checkbox(label=\"Model is already merged\", value=False)\n",
        "        merge_button = gr.Button(\"Merge Model\")\n",
        "        merge_status = gr.Textbox(label=\"Merge Status\", interactive=False)\n",
        "\n",
        "        merge_button.click(fn=merge_model,\n",
        "                           inputs=[model_id, model_is_merged],\n",
        "                           outputs=merge_status)\n",
        "\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\"## Step 2: Convert to GGUF\")\n",
        "        convert_button = gr.Button(\"Convert to GGUF\")\n",
        "\n",
        "        gguf_file = gr.File(label=\"GGUF File (click to download)\")\n",
        "\n",
        "        convert_button.click(fn=convert_to_gguf, inputs=[], outputs=gguf_file)\n",
        "\n",
        "    gr.Markdown(\"**Note:** Ensure that the merged model folder (`./merged_model`) exists before converting.\")\n",
        ""
      ],
      "metadata": {
        "id": "MDCrkyR6RT_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "IbkHhtEfR2qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmDDtwjCXqpw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}