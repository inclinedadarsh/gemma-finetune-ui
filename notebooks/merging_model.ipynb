{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Merging and Converting HF model to GGUF Format\n",
        "\n",
        "In this notebook, we'll be seeing how one can load their model from hugging face, merge that model with the base model and then convert it into GGUF format that can be directly used in Ollama.\n",
        "\n",
        "For this notebook, we'll be loading the [inclinedadarsh/gemma-3-1b-nl-to-regex](https://huggingface.co/inclinedadarsh/gemma-3-1b-nl-to-regex) model, merge it and then finally convert it to GGUF format using [llama.cpp](https://github.com/ggml-org/llama.cpp/)."
      ],
      "metadata": {
        "id": "Olrli8-wCq-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Make sure to change the runtime type to **T4 GPU**"
      ],
      "metadata": {
        "id": "_MHOIT3FKXAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/inclinedadarsh/gemma-finetune-ui/blob/main/notebooks/merging_model.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "dXaSr1HBJvKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U peft transformers"
      ],
      "metadata": {
        "id": "7i8u1-Qs9DP8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMzV1DQxm-3Z"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "wJtCktGAowHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and merging the model"
      ],
      "metadata": {
        "id": "QegfgtPOEjEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"inclinedadarsh/gemma-3-1b-nl-to-regex\""
      ],
      "metadata": {
        "id": "g7-auq4FoCc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = PeftConfig.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "J1wFra4T4wNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = peft_config.base_model_name_or_path"
      ],
      "metadata": {
        "id": "J0FmcI8t40Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation='eager'\n",
        ")"
      ],
      "metadata": {
        "id": "-ewCSdEvoPbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model, model_name)"
      ],
      "metadata": {
        "id": "euW0PQtkohIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's finally merge and unload the model.\n",
        "\n",
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "8xJXbC3uqEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We're going to save the model and the tokenizer in the `merged_model` directory.\n",
        "\n",
        "merged_model.save_pretrained('./merged_model')\n",
        "tokenizer.save_pretrained(\"./merged_model\")"
      ],
      "metadata": {
        "id": "HNVSO18R-S1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting model to GGUF Format (for Ollama)"
      ],
      "metadata": {
        "id": "000_gZgDE3B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "F8R_zeGe5gnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I'll be using the parent directory /content because of the file structure of Google. You might want to change it if you're using Kaggle or doing this locally.\n",
        "\n",
        "%cd /content/llama.cpp/\n",
        "!python convert_hf_to_gguf.py /content/merged_model --outfile /content/merged_model/merged_model.gguf"
      ],
      "metadata": {
        "id": "7nvEvbIPBKzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the model for Ollama"
      ],
      "metadata": {
        "id": "ZnlGGDLgCQGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/merged_model/merged_model.gguf')"
      ],
      "metadata": {
        "id": "TI5kjIuaFMV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_H8CL-4lFRzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}